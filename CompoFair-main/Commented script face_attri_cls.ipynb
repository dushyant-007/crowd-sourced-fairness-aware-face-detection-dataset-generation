{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akgo1\\anaconda3\\lib\\site-packages\\torchvision\\io\\image.py:11: UserWarning: Failed to load image Python extension: Could not find module 'C:\\Users\\akgo1\\anaconda3\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pdb\n",
    "import glob\n",
    "import random\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import tqdm\n",
    "import time\n",
    "import h5py\n",
    "import wandb\n",
    "from PIL import Image\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from tensorboardX import SummaryWriter\n",
    "import torchvision.models as models\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from sklearn.metrics import top_k_accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "from transformers import BertTokenizerFast as BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "import torchvision\n",
    "\n",
    "from utils import cal_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Included parse_args(\"\") - quotes inside\n",
    "\n",
    "def get_parser():\n",
    "\tparser = argparse.ArgumentParser()\n",
    "\tparser.add_argument('--name', type=str, default=''.join(random.choice(string.ascii_lowercase) for i in range(10)))\n",
    "\tparser.add_argument('--seed', type=int, default=7)\n",
    "\tparser.add_argument('--gpu', action='store_true')\n",
    "\tparser.add_argument('--gpus', action='store_true')\n",
    "\n",
    "\tparser.add_argument('--data_path', type=str, default='./dataset/celeba/crowd.csv')\n",
    "\n",
    "\t# data\n",
    "\tparser.add_argument('--train_test_split', type=float, default=0.2)\n",
    "\tparser.add_argument('--train_val_split', type=float, default=0.2)\n",
    "\tparser.add_argument('--classnum', type=int, default=0)\n",
    "\tparser.add_argument('--batch_size', type=int, default=32)\n",
    "\tparser.add_argument('--num_workers', type=int, default=0)\n",
    "\tparser.add_argument('--max_len', type=int, default=300)\n",
    "\tparser.add_argument('--max_vocab', type=int, default=1000)\n",
    "\tparser.add_argument('--img_size', type=int, default=256)\n",
    "\tparser.add_argument('--crop_size', type=int, default=224)\n",
    "\tparser.add_argument('--target', type=str, default='skin')\n",
    "\n",
    "\t# train\n",
    "\tparser.add_argument('--log', action='store_true')\n",
    "\tparser.add_argument('--test', action='store_true')\n",
    "\tparser.add_argument('--save', type=str, default='./save/nsi/base.pth')\n",
    "\tparser.add_argument('--epoches', type=int, default=15)\n",
    "\tparser.add_argument('--base_lr', type=float, default=1e-3)\n",
    "\tparser.add_argument('--train_frac', type=float, default=1.0)\n",
    "\tparser.add_argument('--weight_decay', type=float, default=2e-5)\n",
    "\tparser.add_argument('--dropout', type=float, default=0.1)\n",
    "\tparser.add_argument('--warm_up_split', type=int, default=5)\n",
    "\n",
    "\t# model\n",
    "\tparser.add_argument('--pretrain', action='store_true')\n",
    "\tparser.add_argument('--backbone', type=str, default='vgg16')\n",
    "\tparser.add_argument('--embed_dim', type=int, default=64)\n",
    "\tparser.add_argument('--hidden_dim', type=int, default=32)\n",
    "\tparser.add_argument('--word_embed_size', type=int, default=128)\n",
    "\n",
    "\topt = parser.parse_args(\"\")\n",
    "\n",
    "\treturn opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms images - crops random portions, converts to gray scale with a probability, flips, normalizes etc.\n",
    "\n",
    "class CusDataset(Dataset):\n",
    "\tdef __init__(self, opt, csv, mode='train'):\n",
    "\t\tself.opt = opt\n",
    "\t\tself.csv = csv\n",
    "\t\tself.mode = mode\n",
    "\n",
    "\t\tself.trans = self.get_trans()\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\titem = self.csv.iloc[idx]\n",
    "\t\tpath = item['path']\n",
    "\t\tif not os.path.exists(path):\n",
    "\t\t\tpath = path.replace('pool', 'images')\n",
    "\t\timg = Image.open(path).convert('RGB')\n",
    "\t\timg = self.trans(img)\n",
    "\t\treturn img, item['label_idx'], path\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.csv)\n",
    "\n",
    "\tdef get_trans(self):\n",
    "\t\tnormalize = transforms.Normalize(\n",
    "\t\t\tmean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "\t\tif self.mode == 'train':\n",
    "\t\t\treturn transforms.Compose([\n",
    "\t\t\t\ttransforms.RandomResizedCrop((128, 128)),\n",
    "\t\t\t\ttransforms.RandomGrayscale(p=0.2),\n",
    "\t\t\t\ttransforms.RandomHorizontalFlip(),\n",
    "\t\t\t\ttransforms.ToTensor(),\n",
    "\t\t\t\tnormalize,\n",
    "\t\t\t])\n",
    "\t\telse:\n",
    "\t\t\treturn transforms.Compose([\n",
    "\t\t\t\ttransforms.Resize((128, 128)),\n",
    "\t\t\t\t# transforms.CenterCrop(224),\n",
    "\t\t\t\ttransforms.ToTensor(),\n",
    "\t\t\t\tnormalize,\n",
    "\t\t\t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backbone is a std model used initially (default = 'vgg16')\n",
    "# then data is passed to a linear layer and dropout layer with certain probability (--dropout)\n",
    "\n",
    "class AttriCLS(nn.Module):\n",
    "\tdef __init__(self, args):\n",
    "\t\tsuper(AttriCLS, self).__init__()\n",
    "\t\tself.opt = args\n",
    "\t\tself.backbone, out_feat = self.get_backbone()\n",
    "\t\tself.classifier = nn.Linear(out_feat, opt.classnum)\n",
    "\t\tself.dropout = nn.Dropout(opt.dropout)\n",
    "\n",
    "\tdef forward(self, imgs):\n",
    "\t\t### IMAGE #####\n",
    "\t\timgs = self.backbone(imgs)\n",
    "\t\timgs = self.dropout(imgs.mean(-1).mean(-1))\n",
    "\n",
    "\t\touts = self.classifier(imgs)\n",
    "\n",
    "\t\treturn outs\n",
    "\n",
    "\tdef get_backbone(self):\n",
    "\t\tif self.opt.backbone == 'vgg16':\n",
    "\t\t\tbackbone = nn.Sequential(*list(torchvision.models.vgg16(pretrained=self.opt.pretrain).children())[:-1])\n",
    "# \t\t\tprint('inside class',backbone)\n",
    "\t\t\tout_feat = 512\n",
    "\t\telif self.opt.backbone == 'resnet18':\n",
    "\t\t\tbackbone = nn.Sequential(*list(torchvision.models.resnet18(pretrained=self.opt.pretrain).children())[:-1])\n",
    "\t\t\tout_feat = 512\n",
    "\t\telif self.opt.backbone == 'resnet50':\n",
    "\t\t\tbackbone = nn.Sequential(*list(torchvision.models.resnet50(pretrained=self.opt.pretrain).children())[:-1])\n",
    "\t\t\tout_feat = 2048\n",
    "\t\telif self.opt.backbone == 'alexnet':\n",
    "\t\t\tbackbone = nn.Sequential(*list(torchvision.models.alexnet(pretrained=self.opt.pretrain).children())[:-1])\n",
    "\t\t\tout_feat = 256\n",
    "\t\telif self.opt.backbone == 'squeezenet':\n",
    "\t\t\tbackbone = nn.Sequential(*list(torchvision.models.squeezenet1_0(pretrained=self.opt.pretrain).children())[:-1])\n",
    "\t\t\tout_feat = 512\n",
    "\t\telse:\n",
    "\t\t\traise\n",
    "\n",
    "\t\treturn backbone, out_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target refers to face components to be protected, default is 'skin'\n",
    "\n",
    "def convert_crowd_to_label(opt, csv):\n",
    "\tcsv = csv[csv['compo'] == opt.target].values\n",
    "\tres = []\n",
    "# \tprint(\"Inside func\")\n",
    "# \tdisplay(csv)\n",
    "\tfor line in csv:\n",
    "\t\tres.append([line[line[3]], line[6]]) #Appending the attribute of matching images as given by crowdsource worker\n",
    "# \t\tprint(res)\n",
    "\t\tres.append([line[line[4]], line[6]])\n",
    "# \t\tprint(res)\n",
    "\t\tres.append([line[3-line[3]-line[4]], line[7]])\n",
    "# \t\tprint(res)\n",
    "# \tprint(res)\n",
    "\tres = pd.DataFrame(res, columns=['path', 'label'])\n",
    "\tname2idx = {x:i for i, x in enumerate(sorted(res['label'].unique()))}\n",
    "# \tprint(name2idx) #Dictionary mapping label to description\n",
    "\tres['label_idx'] = res['label'].map(name2idx)\n",
    "# \tprint(res) \n",
    "\treturn res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "      <th>label_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./dataset/celeba/pool/053969.jpg</td>\n",
       "      <td>light</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./dataset/celeba/pool/132912.jpg</td>\n",
       "      <td>light</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./dataset/celeba/pool/052093.jpg</td>\n",
       "      <td>brown</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./dataset/celeba/pool/016652.jpg</td>\n",
       "      <td>brown</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./dataset/celeba/pool/055875.jpg</td>\n",
       "      <td>brown</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>./dataset/celeba/pool/094813.jpg</td>\n",
       "      <td>light</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>./dataset/celeba/pool/076592.jpg</td>\n",
       "      <td>brown</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>./dataset/celeba/pool/154034.jpg</td>\n",
       "      <td>light</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>./dataset/celeba/pool/076325.jpg</td>\n",
       "      <td>light</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>./dataset/celeba/pool/008601.jpg</td>\n",
       "      <td>dark</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>285 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 path  label  label_idx\n",
       "0    ./dataset/celeba/pool/053969.jpg  light          3\n",
       "1    ./dataset/celeba/pool/132912.jpg  light          3\n",
       "2    ./dataset/celeba/pool/052093.jpg  brown          0\n",
       "3    ./dataset/celeba/pool/016652.jpg  brown          0\n",
       "4    ./dataset/celeba/pool/055875.jpg  brown          0\n",
       "..                                ...    ...        ...\n",
       "280  ./dataset/celeba/pool/094813.jpg  light          3\n",
       "281  ./dataset/celeba/pool/076592.jpg  brown          0\n",
       "282  ./dataset/celeba/pool/154034.jpg  light          3\n",
       "283  ./dataset/celeba/pool/076325.jpg  light          3\n",
       "284  ./dataset/celeba/pool/008601.jpg   dark          1\n",
       "\n",
       "[285 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x0000024AB2C63EB0>\n",
      "model parameters: classifier.bias [Parameter containing:\n",
      "tensor([[-0.0075, -0.0335, -0.0369,  ...,  0.0412,  0.0079,  0.0160],\n",
      "        [ 0.0063,  0.0180, -0.0107,  ...,  0.0256,  0.0397,  0.0055],\n",
      "        [-0.0385,  0.0276, -0.0199,  ...,  0.0317, -0.0418, -0.0042],\n",
      "        [-0.0292, -0.0046, -0.0073,  ..., -0.0224,  0.0224, -0.0242]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0066, -0.0001,  0.0134,  0.0437], requires_grad=True)]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-13ae0c4491a2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     61\u001b[0m                                 \u001b[1;32mif\u001b[0m \u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgpu\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m                                         \u001b[0mimgs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                                 \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m                                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-6f3626135158>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, imgs)\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimgs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m                 \u001b[1;31m### IMAGE #####\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m                 \u001b[0mimgs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m                 \u001b[0mimgs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 446\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    440\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m--> 442\u001b[1;33m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[0;32m    443\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\topt = get_parser()\n",
    "# \tprint(opt)\n",
    "\tcsv = pd.read_csv(opt.data_path)\n",
    "# \tdisplay(csv)\n",
    "\tcsv = convert_crowd_to_label(opt, csv)\n",
    "\tdisplay(csv)\n",
    "\tcsv.to_csv(\"List of images.csv\")   \n",
    "\tlist_of_images = csv['path'].str.split('/').str[-1]\n",
    "\tlist_of_images.to_csv(\"List of images.csv\")\n",
    "# \tprint(list_of_images)    \n",
    "# \tdisplay(images)\n",
    "\n",
    "\topt.classnum = csv['label'].nunique()\n",
    "\tif opt.log:\n",
    "\t\twandb.init(project=\"CompoFair\", name=opt.name)\n",
    "\t\twandb.config.update(opt)\n",
    "\n",
    "\ttrain, test = train_test_split(csv, test_size=opt.train_test_split, stratify=csv['label'], random_state=42)\n",
    "\n",
    "\ttrain_set = CusDataset(opt, train, mode='train')\n",
    "\ttest_set = CusDataset(opt, test, mode='test')\n",
    "\n",
    "\ttrainloader = DataLoader(train_set, batch_size=opt.batch_size,\n",
    "\t\t\t\t\t\t\t shuffle=True, num_workers=opt.num_workers, drop_last=True)\n",
    "\ttestloader = DataLoader(test_set, batch_size=opt.batch_size,\n",
    "\t\t\t\t\t\t   shuffle=False, num_workers=opt.num_workers)\n",
    "\tprint(trainloader)    \n",
    "    \n",
    "\tmodel = AttriCLS(opt)\n",
    "\tif opt.gpu:\n",
    "\t\tmodel = model.cuda()\n",
    "        \n",
    "\t#Assigning weights to classes to avoid skewness of distribution\n",
    "    \n",
    "\tloss_func = nn.CrossEntropyLoss(cal_weights(train, 'label').cuda() if opt.gpu else cal_weights(train, 'label'))\n",
    "    \n",
    "\tparams, base_params = [], []\n",
    "\tfor name, param in model.named_parameters():\n",
    "\t\tif 'backbone' in name:\n",
    "\t\t\tbase_params.append(param)\n",
    "\t\telse:\n",
    "\t\t\tparams.append(param)\n",
    "\tprint('model parameters:', name, params)\n",
    "    \n",
    "\toptimizer = optim.AdamW([{'params': params}, {'params': base_params, 'lr': opt.base_lr/10}], lr=opt.base_lr, weight_decay=opt.weight_decay)\n",
    "\ttotal_training_steps = len(trainloader) * opt.epoches\n",
    "\twarmup_steps = total_training_steps // opt.warm_up_split\n",
    "\tscheduler = get_linear_schedule_with_warmup(\n",
    "\t\toptimizer,\n",
    "\t\tnum_warmup_steps=warmup_steps,\n",
    "\t\tnum_training_steps=total_training_steps\n",
    "\t)\n",
    "\n",
    "\tif opt.test:\n",
    "\t\tbest_model, best_loss = torch.load(opt.save), -1\n",
    "\telse:\n",
    "\t\tbest_model, best_loss = model.state_dict(), 1e5\n",
    "        \n",
    "\t\tfor epo in range(opt.epoches):\n",
    "\t\t\tmodel.train()\n",
    "\t\t\ttrain_loss = 0.\n",
    "\t\t\tfor step, pack in enumerate(trainloader):\n",
    "\t\t\t\timgs, targets, _ = pack\n",
    "# \t\t\t\tprint('imgs', imgs.size(), 'targets', targets.size()) batch_size=32             \n",
    "\t\t\t\tif opt.gpu:\n",
    "\t\t\t\t\timgs, targets = imgs.cuda(), targets.cuda()\n",
    "\t\t\t\tout = model(imgs)\n",
    "\t\t\t\tloss = loss_func(out, targets)\n",
    "\n",
    "\t\t\t\toptimizer.zero_grad()\n",
    "\t\t\t\tloss.backward()\n",
    "\t\t\t\toptimizer.step()\n",
    "\t\t\t\tscheduler.step()\n",
    "\n",
    "\t\t\t\ttrain_loss += loss.item()\n",
    "\n",
    "\t\t\tif opt.log:\n",
    "\t\t\t\twandb.log({'train_loss': train_loss/len(trainloader)})\n",
    "\n",
    "\t\t\tmodel.eval()\n",
    "\t\t\teval_loss = 0.\n",
    "\t\t\tfor step, pack in enumerate(testloader):\n",
    "\t\t\t\timgs, targets, _ = pack\n",
    "\t\t\t\tif opt.gpu:\n",
    "\t\t\t\t\timgs, targets = imgs.cuda(), targets.cuda()\n",
    "\t\t\t\tout = model(imgs)\n",
    "\t\t\t\teval_loss += loss_func(out, targets).item()\n",
    "\n",
    "\t\t\tif opt.log:\n",
    "\t\t\t\twandb.log({'val_loss': eval_loss/len(testloader)})\n",
    "\n",
    "\t\t\tif eval_loss < best_loss:\n",
    "\t\t\t\tbest_model, best_loss = model.state_dict(), eval_loss\n",
    "\n",
    "\tmodel.load_state_dict(best_model)\n",
    "\ttarget_preds, target_gts, paths = [], [], []\n",
    "\tmodel.eval()\n",
    "\twith torch.no_grad():\n",
    "\t\tfor step, pack in enumerate(testloader):\n",
    "\t\t\timgs, targets, path = pack\n",
    "\t\t\tif opt.gpu:\n",
    "\t\t\t\timgs, targets = imgs.cuda(), targets\n",
    "\t\t\tout = model(imgs)\n",
    "\t\t\ttarget_preds.append(out.cpu().numpy())\n",
    "\t\t\ttarget_gts.append(targets.numpy())\n",
    "\t\t\tpaths.append(path)\n",
    "\ttarget_preds = np.concatenate(target_preds, 0)\n",
    "\ttarget_gts = np.concatenate(target_gts, 0)\n",
    "\tpaths = np.concatenate(paths, 0)\n",
    "\n",
    "\tprint('top1: {} top2: {}'.format(top_k_accuracy_score(target_gts, target_preds, k=1), top_k_accuracy_score(target_gts, target_preds, k=2)))\n",
    "\n",
    "\tif not opt.test:\n",
    "\t\ttorch.save(model.state_dict(), './save/base_skin.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
